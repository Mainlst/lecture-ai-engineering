import os
import pytest
import pandas as pd
import numpy as np
import pickle
import time
import json # JSONã‚’æ‰±ã†ãŸã‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings

warnings.filterwarnings("ignore", category=UserWarning, module='sklearn')
warnings.filterwarnings("ignore", category=FutureWarning, module='sklearn')


# --- ãƒ‘ã‚¹å®šç¾© ---
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’åŸºæº–ã«ã€ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆã‚„ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã‚’æ±ºå®šã—ã¾ã™ã€‚
# day5/æ¼”ç¿’3/tests/test_model.py ã‹ã‚‰è¦‹ã¦ã€
# ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆã¯3ã¤ä¸Šã®éšå±¤ (../../..)
# ãƒ‡ãƒ¼ã‚¿ã¯ ../data/Titanic.csv
# ãƒ¢ãƒ‡ãƒ«ã¯ ../models/titanic_model.pkl
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆç›´ä¸‹ã« current_metrics.json ã¨ã—ã¦ä¿å­˜ã™ã‚‹æƒ³å®š
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) # ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆ
DATA_PATH = os.path.join(os.path.dirname(__file__), "../data/Titanic.csv")
MODEL_DIR = os.path.join(os.path.dirname(__file__), "../models")
MODEL_PATH = os.path.join(MODEL_DIR, "titanic_model.pkl")

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆç›´ä¸‹ã«ä¿å­˜/å‚ç…§ã™ã‚‹æƒ³å®š)
CURRENT_METRICS_PATH = os.path.join(BASE_DIR, "current_metrics.json")
# GitHub Actionsã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸéå»ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
# (ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã® download-artifact ã® path ã¨ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆåã«åˆã‚ã›ã‚‹)
PREVIOUS_METRICS_ARTIFACT_DIR = os.path.join(BASE_DIR, "previous_metrics_artifact")
PREVIOUS_METRICS_PATH = os.path.join(PREVIOUS_METRICS_ARTIFACT_DIR, "current_metrics.json") # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã¨åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«å

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° (ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ç”¨) ---
collected_metrics = {}

@pytest.fixture(scope="session") # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ã‚³ãƒ¼ãƒ—ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚€
def raw_data():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ (ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)"""
    if not os.path.exists(DATA_PATH):
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« {DATA_PATH} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã¾ã™...")
        from sklearn.datasets import fetch_openml
        # parser='auto' ã‚’æŒ‡å®šã—ã¦è­¦å‘Šã‚’æŠ‘åˆ¶
        titanic_data = fetch_openml("titanic", version=1, as_frame=True, parser='auto')
        df = titanic_data.frame
        # fetch_openmlã® 'survived' ã‚«ãƒ©ãƒ ã¯ã‚«ãƒ†ã‚´ãƒªå‹ (object) ã§ '0', '1' ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹
        # ã“ã‚Œã‚’æ•°å€¤ã«å¤‰æ›ã—ã¦ãŠã
        # df['Survived'] = pd.to_numeric(df['survived']) # 'survived' ã¯å°æ–‡å­—ã‹ã‚‚ã—ã‚Œãªã„

        # ã‚«ãƒ©ãƒ åã‚’å°æ–‡å­—ã«çµ±ä¸€ï¼ˆã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ï¼‰
        df.columns = [col.lower() for col in df.columns]

        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿é¸æŠ (å°æ–‡å­—ã§)
        required_cols_raw = ["pclass", "sex", "age", "sibsp", "parch", "fare", "embarked", "survived"]
        missing_cols = [col for col in required_cols_raw if col not in df.columns]
        if missing_cols:
            raise ValueError(f"å¿…è¦ãªã‚«ãƒ©ãƒ  {missing_cols} ãŒDataFrameã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {df.columns.tolist()}")

        df_selected = df[required_cols_raw]

        os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)
        df_selected.to_csv(DATA_PATH, index=False)
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {DATA_PATH} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return df_selected
    
    df_loaded = pd.read_csv(DATA_PATH)
    df_loaded.columns = [col.lower() for col in df_loaded.columns] # èª­ã¿è¾¼ã‚“ã CSVã‚‚ã‚«ãƒ©ãƒ åã‚’å°æ–‡å­—ã«
    return df_loaded


@pytest.fixture(scope="session")
def preprocessor(raw_data): # raw_dataãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã«ä¾å­˜
    """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®šç¾©"""
    numeric_features = ["age", "pclass", "sibsp", "parch", "fare"]
    categorical_features = ["sex", "embarked"]

    # raw_data ã«æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    for col in numeric_features + categorical_features:
        if col not in raw_data.columns:
            raise ValueError(f"å‰å‡¦ç†ã«å¿…è¦ãªã‚«ãƒ©ãƒ  '{col}' ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {raw_data.columns.tolist()}")

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )
    preprocessor_obj = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )
    return preprocessor_obj


@pytest.fixture(scope="session") # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ã‚³ãƒ¼ãƒ—ã§ä¸€åº¦ã ã‘
def trained_model_and_test_data(raw_data, preprocessor): # raw_dataã¨preprocessorã«ä¾å­˜
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
    if "survived" not in raw_data.columns:
        raise ValueError(f"ç›®çš„å¤‰æ•° 'survived' ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {raw_data.columns.tolist()}")

    X = raw_data.drop("survived", axis=1)
    y = raw_data["survived"].astype(int) # survivedã‚’intå‹ã«å¤‰æ›
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¿½åŠ 
    )

    model = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("classifier", RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')), # class_weightã‚’è¿½åŠ 
        ]
    )
    model.fit(X_train, y_train)

    os.makedirs(MODEL_DIR, exist_ok=True)
    with open(MODEL_PATH, "wb") as f:
        pickle.dump(model, f)
    
    return model, X_test, y_test


def test_model_exists():
    """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
    # trained_model_and_test_data ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ãŒå®Ÿè¡Œã•ã‚Œã‚Œã°ãƒ¢ãƒ‡ãƒ«ã¯ä¿å­˜ã•ã‚Œã‚‹ã¯ãš
    assert os.path.exists(MODEL_PATH), f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« {MODEL_PATH} ãŒå­˜åœ¨ã—ã¾ã›ã‚“"


def test_model_accuracy(trained_model_and_test_data):
    """ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’æ¤œè¨¼ã—ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
    model, X_test, y_test = trained_model_and_test_data
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦: {accuracy:.4f}")
    collected_metrics["accuracy"] = accuracy
    assert accuracy >= 0.70, f"ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ ({accuracy:.4f}) ãŒæœŸå¾…å€¤ (0.70) ã‚ˆã‚Šä½ã„ã§ã™" # é–¾å€¤ã‚’å°‘ã—ç¾å®Ÿçš„ã«


def test_model_inference_time(trained_model_and_test_data):
    """ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–æ™‚é–“ã‚’æ¤œè¨¼ã—ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
    model, X_test, _ = trained_model_and_test_data # y_testã¯ä¸è¦
    
    if X_test.empty:
        print("æ¨è«–ç”¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€æ¨è«–æ™‚é–“ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        collected_metrics["inference_time_total_seconds"] = 0
        collected_metrics["inference_time_avg_ms_per_sample"] = 0
        pytest.skip("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return

    start_time = time.time()
    model.predict(X_test)
    end_time = time.time()
    inference_time_total = end_time - start_time
    inference_time_avg_ms = (inference_time_total / len(X_test)) * 1000 if len(X_test) > 0 else 0
    
    print(f"ç·æ¨è«–æ™‚é–“: {inference_time_total:.4f}ç§’ ({len(X_test)}ä»¶)")
    print(f"1ä»¶ã‚ãŸã‚Šã®å¹³å‡æ¨è«–æ™‚é–“: {inference_time_avg_ms:.4f}ãƒŸãƒªç§’")
    collected_metrics["inference_time_total_seconds"] = inference_time_total
    collected_metrics["inference_time_avg_ms_per_sample"] = inference_time_avg_ms
    # éå¸¸ã«å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã®ã§ã€æ¨è«–æ™‚é–“ã¯ã‹ãªã‚ŠçŸ­ããªã‚‹ã¯ãš
    assert inference_time_total < 5.0, f"ç·æ¨è«–æ™‚é–“ ({inference_time_total:.4f}ç§’) ãŒé•·ã™ãã¾ã™"


def test_model_reproducibility(raw_data, preprocessor): # trained_model_and_test_dataã¯ä½¿ã‚ãªã„
    """ãƒ¢ãƒ‡ãƒ«ã®å†ç¾æ€§ã‚’æ¤œè¨¼ (åŒã˜ãƒ‡ãƒ¼ã‚¿ãƒ»è¨­å®šãªã‚‰åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒã§ãã‚‹ã‹)"""
    X = raw_data.drop("survived", axis=1)
    y = raw_data["survived"].astype(int)
    X_train, X_test, y_train, _ = train_test_split( # y_testã¯ä¸è¦
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    model1 = Pipeline(
        steps=[
            ("preprocessor", preprocessor), # preprocessorã‚’å†åˆ©ç”¨
            ("classifier", RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')),
        ]
    )
    model1.fit(X_train, y_train)
    predictions1 = model1.predict(X_test)

    # preprocessor ã¯ fit ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€æ–°ã—ã„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯å†åº¦ fit ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
    # ã‚‚ã—ãã¯ã€preprocessor ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¯å›æ–°ã—ãä½œã‚‹
    # ã“ã“ã§ã¯ã€åŒã˜ preprocessor ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ã„ã€å†åº¦ fit ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
    
    # æ¯å›æ–°ã—ã„å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã™ã‚‹æ–¹ãŒå®‰å…¨
    numeric_features = ["age", "pclass", "sibsp", "parch", "fare"]
    categorical_features = ["sex", "embarked"]
    numeric_transformer_new = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")),("scaler", StandardScaler())])
    categorical_transformer_new = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")),("onehot", OneHotEncoder(handle_unknown="ignore"))])
    preprocessor_new = ColumnTransformer(transformers=[("num", numeric_transformer_new, numeric_features),("cat", categorical_transformer_new, categorical_features)])

    model2 = Pipeline(
        steps=[
            ("preprocessor", preprocessor_new), # æ–°ã—ã„ preprocessor ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            ("classifier", RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')),
        ]
    )
    model2.fit(X_train, y_train)
    predictions2 = model2.predict(X_test)

    assert np.array_equal(predictions1, predictions2), "ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœã«å†ç¾æ€§ãŒã‚ã‚Šã¾ã›ã‚“"


def test_performance_regression():
    """éå»ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦æ€§èƒ½ãŒåŠ£åŒ–ã—ã¦ã„ãªã„ã‹æ¤œè¨¼"""
    if not os.path.exists(CURRENT_METRICS_PATH):
        pytest.skip(f"ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ« ({CURRENT_METRICS_PATH}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    with open(CURRENT_METRICS_PATH, "r") as f:
        current_metrics = json.load(f)
    
    current_accuracy = current_metrics.get("accuracy")
    current_inference_time_avg = current_metrics.get("inference_time_avg_ms_per_sample")

    if not os.path.exists(PREVIOUS_METRICS_PATH):
        print(f"éå»ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ« ({PREVIOUS_METRICS_PATH}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆå›å®Ÿè¡Œã¾ãŸã¯æ¯”è¼ƒå¯¾è±¡ãªã—ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚")
        assert current_accuracy is not None, "ç¾åœ¨ã®ç²¾åº¦ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        pytest.skip("éå»ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ã€æ€§èƒ½æ¯”è¼ƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    with open(PREVIOUS_METRICS_PATH, "r") as f:
        previous_metrics = json.load(f)

    previous_accuracy = previous_metrics.get("accuracy")
    previous_inference_time_avg = previous_metrics.get("inference_time_avg_ms_per_sample")

    print(f"æ€§èƒ½æ¯”è¼ƒ - ç²¾åº¦:      ç¾åœ¨={current_accuracy:.4f}, éå»={previous_accuracy:.4f if previous_accuracy else 'N/A'}")
    print(f"æ€§èƒ½æ¯”è¼ƒ - å¹³å‡æ¨è«–æ™‚é–“: ç¾åœ¨={current_inference_time_avg:.4f}ms, éå»={previous_inference_time_avg:.4f if previous_inference_time_avg else 'N/A'}ms")

    if previous_accuracy is not None and current_accuracy is not None:
        # ç²¾åº¦ãŒè‘—ã—ãä½ä¸‹ã—ã¦ã„ãªã„ã‹ (ä¾‹: éå»ã®95%æœªæº€ã«ãªã£ãŸã‚‰ã‚¨ãƒ©ãƒ¼)
        assert current_accuracy >= previous_accuracy * 0.95, \
            f"ç²¾åº¦ãŒè¨±å®¹ç¯„å›²ã‚’è¶…ãˆã¦ä½ä¸‹ã—ã¾ã—ãŸã€‚ç¾åœ¨: {current_accuracy:.4f}, éå»: {previous_accuracy:.4f}"

    if previous_inference_time_avg is not None and current_inference_time_avg is not None and previous_inference_time_avg > 0:
        # æ¨è«–æ™‚é–“ãŒè‘—ã—ãå¢—åŠ ã—ã¦ã„ãªã„ã‹ (ä¾‹: éå»ã®1.5å€ã‚’è¶…ãˆãŸã‚‰ã‚¨ãƒ©ãƒ¼)
        assert current_inference_time_avg <= previous_inference_time_avg * 1.5, \
            f"å¹³å‡æ¨è«–æ™‚é–“ãŒè¨±å®¹ç¯„å›²ã‚’è¶…ãˆã¦å¢—åŠ ã—ã¾ã—ãŸã€‚ç¾åœ¨: {current_inference_time_avg:.4f}ms, éå»: {previous_inference_time_avg:.4f}ms"

# --- pytest ãƒ•ãƒƒã‚¯ (ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«å®Ÿè¡Œ) ---
def pytest_sessionfinish(session, exitstatus):
    """ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ€å¾Œã«åé›†ã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™"""
    if exitstatus == 0 and collected_metrics: # ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã€ã‹ã¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåé›†ã•ã‚Œã¦ã„ã‚‹å ´åˆ
        # (æ³¨æ„: pytest_sessionfinish ã¯å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒçµ‚ã‚ã£ãŸå¾Œã«å‘¼ã°ã‚Œã‚‹ã®ã§ã€
        #  ã“ã“ã«æ¥ã‚‹ã¾ã§ã« collected_metrics ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)
        print(f"\nãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ã€‚åé›†ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹: {collected_metrics}")
        try:
            with open(CURRENT_METRICS_PATH, "w") as f:
                json.dump(collected_metrics, f, indent=4)
            print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ {CURRENT_METRICS_PATH} ã«ä¿å­˜ã—ã¾ã—ãŸï¼ğŸ‰")
        except Exception as e:
            print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    elif not collected_metrics:
        print("\nãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯åé›†ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        print(f"\nãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸãŸã‚ ({exitstatus})ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")